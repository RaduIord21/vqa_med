{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146c9544",
   "metadata": {},
   "source": [
    "# VQA Medical - Training on Google Colab\n",
    "\n",
    "This notebook allows you to train the VQA Medical model on Google Colab using the modular codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e3b67",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f77101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34353e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision transformers tqdm matplotlib pillow -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661a728",
   "metadata": {},
   "source": [
    "## 2. Upload/Clone Project\n",
    "\n",
    "**Option A**: Clone from GitHub (if you pushed the repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Clone from GitHub\n",
    "# !git clone https://github.com/YOUR_USERNAME/vqa_med.git\n",
    "# %cd vqa_med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1de20f",
   "metadata": {},
   "source": [
    "**Option B**: Upload the vqamed package manually\n",
    "\n",
    "Upload the `vqamed/` folder to Colab using the file browser on the left, or run the cell below to create the package inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Create the package inline (run this if you didn't clone/upload)\n",
    "import os\n",
    "os.makedirs('vqamed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vqamed/__init__.py\n",
    "\"\"\"VQA Medical - Visual Question Answering for Medical Images\"\"\"\n",
    "\n",
    "from .config import Config\n",
    "from .dataset import VQADataset, parse_qa_set\n",
    "from .encoders import VisualEncoder, TextEncoder\n",
    "from .fusion import CrossAttentionFusion\n",
    "from .decoder import AnswerDecoder\n",
    "from .model import VQAModel\n",
    "from .training import train_epoch, validate_epoch, EarlyStopping\n",
    "\n",
    "__all__ = [\n",
    "    \"Config\",\n",
    "    \"VQADataset\",\n",
    "    \"parse_qa_set\",\n",
    "    \"VisualEncoder\",\n",
    "    \"TextEncoder\",\n",
    "    \"CrossAttentionFusion\",\n",
    "    \"AnswerDecoder\",\n",
    "    \"VQAModel\",\n",
    "    \"train_epoch\",\n",
    "    \"validate_epoch\",\n",
    "    \"EarlyStopping\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vqamed/config.py\n",
    "\"\"\"Configuration for VQA Medical model.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for VQA Medical training.\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    train_path: str = \"data/Training\"\n",
    "    validation_path: str = \"data/Validation\"\n",
    "    test_path: str = \"data/Test\"\n",
    "    save_path: str = \"checkpoints/best_model.pt\"\n",
    "    \n",
    "    # Model\n",
    "    embed_dim: int = 512\n",
    "    num_heads: int = 8\n",
    "    decoder_layers: int = 4\n",
    "    max_len: int = 32\n",
    "    text_model_name: str = \"bert-base-uncased\"\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 30\n",
    "    learning_rate: float = 1e-4\n",
    "    patience: int = 5\n",
    "    \n",
    "    # Device\n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Create directories if they don't exist.\"\"\"\n",
    "        Path(self.save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    @property\n",
    "    def train_images_dir(self) -> str:\n",
    "        return f\"{self.train_path}/images\"\n",
    "    \n",
    "    @property\n",
    "    def train_qa_file(self) -> str:\n",
    "        return f\"{self.train_path}/all_qa_pairs.txt\"\n",
    "    \n",
    "    @property\n",
    "    def val_images_dir(self) -> str:\n",
    "        return f\"{self.validation_path}/images\"\n",
    "    \n",
    "    @property\n",
    "    def val_qa_file(self) -> str:\n",
    "        return f\"{self.validation_path}/all_qa_pairs.txt\"\n",
    "    \n",
    "    @property\n",
    "    def test_images_dir(self) -> str:\n",
    "        return f\"{self.test_path}/images\"\n",
    "    \n",
    "    @property\n",
    "    def test_qa_file(self) -> str:\n",
    "        return f\"{self.test_path}/questions_w_ref_answers.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vqamed/dataset.py\n",
    "\"\"\"Dataset module for VQA Medical.\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Tuple, List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def parse_qa_set(qa_pairs_txt_path: str) -> Tuple[int, List[Dict[str, str]]]:\n",
    "    \"\"\"Parse QA pairs from a text file.\"\"\"\n",
    "    qa_set = []\n",
    "\n",
    "    with open(qa_pairs_txt_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            elements = line.strip().split('|')\n",
    "            if len(elements) > 3:\n",
    "                image_id = elements[0]\n",
    "                question = elements[2]\n",
    "                answer = elements[3]\n",
    "            else:\n",
    "                image_id, question, answer = elements\n",
    "            qa_set.append({\n",
    "                'image_id': image_id,\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            })\n",
    "\n",
    "    return len(qa_set), qa_set\n",
    "\n",
    "\n",
    "class VQADataset(Dataset):\n",
    "    \"\"\"Dataset for Visual Question Answering on medical images.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir: str,\n",
    "        qa_file: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        max_len: int = 32\n",
    "    ):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.length, self.items = parse_qa_set(qa_file)\n",
    "        self.transform = transform or self._default_transform()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_transform() -> transforms.Compose:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        item = self.items[idx]\n",
    "        img_path = self.images_dir / f\"{item['image_id']}.jpg\"\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        question = item['question']\n",
    "        if self.tokenizer:\n",
    "            tokens = self.tokenizer(\n",
    "                question,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_len,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = tokens['input_ids'].squeeze(0)\n",
    "            attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        else:\n",
    "            input_ids = question\n",
    "            attention_mask = None\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'answer': item['answer']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vqamed/encoders.py\n",
    "\"\"\"Encoder modules for VQA Medical.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    \"\"\"Visual encoder using DenseNet121 backbone.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int = 512):\n",
    "        super().__init__()\n",
    "        base_model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        self.features = base_model.features\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(1024, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x).view(x.size(0), -1)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Text encoder using BERT backbone.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'bert-base-uncased', embed_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.transformer.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = output.last_hidden_state[:, 0]\n",
    "        return self.projection(cls_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vqamed/fusion.py\n",
    "\"\"\"Fusion module for VQA Medical.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"Cross-attention fusion layer for multimodal features.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int = 512, heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        text_embeds: torch.Tensor,\n",
    "        image_embeds: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        attn_output, _ = self.cross_attn(\n",
    "            query=text_embeds,\n",
    "            key=image_embeds,\n",
    "            value=image_embeds\n",
    "        )\n",
    "        x = self.norm(attn_output + text_embeds)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vqamed/decoder.py\n",
    "\"\"\"Answer decoder module for VQA Medical.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AnswerDecoder(nn.Module):\n",
    "    \"\"\"Transformer decoder for generating answers.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int = 512,\n",
    "        max_len: int = 32,\n",
    "        num_layers: int = 4,\n",
    "        nhead: int = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "        self.transformer = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=nhead,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt_input_ids: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        B, T = tgt_input_ids.size()\n",
    "        device = tgt_input_ids.device\n",
    "\n",
    "        pos_ids = torch.arange(T, device=device).unsqueeze(0).expand(B, -1)\n",
    "        tgt_emb = self.token_emb(tgt_input_ids) + self.pos_emb(pos_ids)\n",
    "\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones((T, T), device=device),\n",
    "            diagonal=1\n",
    "        ).bool()\n",
    "\n",
    "        out = self.transformer(\n",
    "            tgt=tgt_emb,\n",
    "            memory=memory,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7341cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vqamed/model.py\n",
    "\"\"\"Main VQA model combining all components.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from .encoders import VisualEncoder, TextEncoder\n",
    "from .fusion import CrossAttentionFusion\n",
    "from .decoder import AnswerDecoder\n",
    "\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    \"\"\"Complete VQA model for medical image question answering.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        visual_encoder: VisualEncoder,\n",
    "        text_encoder: TextEncoder,\n",
    "        fusion: CrossAttentionFusion,\n",
    "        decoder: AnswerDecoder\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.visual_encoder = visual_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.fusion = fusion\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        img_feat = self.visual_encoder(image)\n",
    "        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        fused = self.fusion(text_out, img_feat)\n",
    "        if fused.dim() == 2:\n",
    "            fused = fused.unsqueeze(1)\n",
    "            \n",
    "        logits = self.decoder(\n",
    "            tgt_input_ids=decoder_input_ids,\n",
    "            memory=fused\n",
    "        )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, vocab_size: int) -> \"VQAModel\":\n",
    "        visual_encoder = VisualEncoder(embed_dim=config.embed_dim)\n",
    "        text_encoder = TextEncoder(\n",
    "            model_name=config.text_model_name,\n",
    "            embed_dim=config.embed_dim\n",
    "        )\n",
    "        fusion = CrossAttentionFusion(\n",
    "            dim=config.embed_dim,\n",
    "            heads=config.num_heads\n",
    "        )\n",
    "        decoder = AnswerDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=config.embed_dim,\n",
    "            max_len=config.max_len,\n",
    "            num_layers=config.decoder_layers,\n",
    "            nhead=config.num_heads\n",
    "        )\n",
    "        \n",
    "        return cls(visual_encoder, text_encoder, fusion, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vqamed/training.py\n",
    "\"\"\"Training utilities for VQA Medical.\"\"\"\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    tokenizer: Any,\n",
    "    device: torch.device,\n",
    "    max_len: int = 32,\n",
    "    log_interval: int = 20\n",
    ") -> float:\n",
    "    \"\"\"Train model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for cnt, batch in enumerate(dataloader):\n",
    "        if cnt % log_interval == 0:\n",
    "            print(f'{cnt} / {len(dataloader)}')\n",
    "            \n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        answers = batch['answer']\n",
    "\n",
    "        answer_tokens = tokenizer(\n",
    "            answers,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        decoder_input_ids = answer_tokens['input_ids'][:, :-1].to(device)\n",
    "        labels = answer_tokens['input_ids'][:, 1:].to(device)\n",
    "\n",
    "        logits = model(\n",
    "            image=images,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            labels.reshape(-1),\n",
    "            ignore_index=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    tokenizer: Any,\n",
    "    device: torch.device,\n",
    "    max_len: int = 32,\n",
    "    log_interval: int = 20\n",
    ") -> float:\n",
    "    \"\"\"Validate model for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for cnt, batch in enumerate(dataloader):\n",
    "        if cnt % log_interval == 0:\n",
    "            print(f\"{cnt} / {len(dataloader)}\")\n",
    "            \n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        answers = batch['answer']\n",
    "\n",
    "        answer_tokens = tokenizer(\n",
    "            answers,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        decoder_input_ids = answer_tokens['input_ids'][:, :-1].to(device)\n",
    "        labels = answer_tokens['input_ids'][:, 1:].to(device)\n",
    "\n",
    "        logits = model(images, input_ids, attention_mask, decoder_input_ids)\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            labels.reshape(-1),\n",
    "            ignore_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping handler.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 5, min_delta: float = 0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd82ee",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive & Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: If dataset is on Google Drive\n",
    "# DATA_PATH = \"/content/drive/MyDrive/datasets/ImageClef-2019-VQA-Med\"\n",
    "\n",
    "# Option 2: If using Kaggle dataset, download it first\n",
    "# !pip install kaggle -q\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets download -d ammar111/imageclef-2019-vqa-med\n",
    "# !unzip imageclef-2019-vqa-med.zip -d data/\n",
    "\n",
    "# Set your data path here:\n",
    "DATA_PATH = \"/content/drive/MyDrive/datasets/ImageClef-2019-VQA-Med\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd0b6b",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96133372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vqamed import Config\n",
    "\n",
    "# Configure paths and hyperparameters\n",
    "config = Config(\n",
    "    train_path=f\"{DATA_PATH}/Training\",\n",
    "    validation_path=f\"{DATA_PATH}/Validation\",\n",
    "    test_path=f\"{DATA_PATH}/Test\",\n",
    "    save_path=\"/content/drive/MyDrive/vqa_med_checkpoints/best_model.pt\",\n",
    "    \n",
    "    # Model params\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    decoder_layers=4,\n",
    "    max_len=32,\n",
    "    \n",
    "    # Training params\n",
    "    batch_size=32,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-4,\n",
    "    patience=5,\n",
    ")\n",
    "\n",
    "device = torch.device(config.device)\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Train path: {config.train_images_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc55548",
   "metadata": {},
   "source": [
    "## 5. Create Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from vqamed import VQADataset\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.text_model_name)\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VQADataset(\n",
    "    images_dir=config.train_images_dir,\n",
    "    qa_file=config.train_qa_file,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config.max_len\n",
    ")\n",
    "\n",
    "val_dataset = VQADataset(\n",
    "    images_dir=config.val_images_dir,\n",
    "    qa_file=config.val_qa_file,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config.max_len\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65d859",
   "metadata": {},
   "source": [
    "## 6. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqamed import VQAModel\n",
    "\n",
    "# Create model from config\n",
    "model = VQAModel.from_config(config, vocab_size=len(tokenizer))\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd52b85",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3505c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from vqamed import train_epoch, validate_epoch, EarlyStopping\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience=config.patience)\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(f\"Starting training for {config.num_epochs} epochs...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in tqdm(range(config.num_epochs), desc=\"Training\"):\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, tokenizer, device, config.max_len\n",
    "    )\n",
    "    val_loss = validate_epoch(\n",
    "        model, val_loader, tokenizer, device, config.max_len\n",
    "    )\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}: Train Loss = {train_loss:.4f} | Val Loss = {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    improved = early_stopping(val_loss)\n",
    "    if improved:\n",
    "        Path(config.save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(model.state_dict(), config.save_path)\n",
    "        print(\"Model improved. Saving.\")\n",
    "    else:\n",
    "        print(f\"No improvement. Patience: {early_stopping.counter}/{config.patience}\")\n",
    "    \n",
    "    if early_stopping.should_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining complete! Best model saved to: {config.save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55261b7",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64234ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "plt.plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training vs Validation Loss', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12493ea2",
   "metadata": {},
   "source": [
    "## 9. Load Best Model & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(config.save_path))\n",
    "model.eval()\n",
    "print(\"Best model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_answer(model, image, question, tokenizer, device, max_len=32):\n",
    "    \"\"\"Generate answer for a given image and question.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize question\n",
    "    tokens = tokenizer(\n",
    "        question,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Start with [CLS] token\n",
    "    decoder_input = torch.tensor([[tokenizer.cls_token_id]]).to(device)\n",
    "    \n",
    "    generated = []\n",
    "    for _ in range(max_len):\n",
    "        logits = model(image, input_ids, attention_mask, decoder_input)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.sep_token_id:\n",
    "            break\n",
    "            \n",
    "        generated.append(next_token.item())\n",
    "        decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "sample = val_dataset[0]\n",
    "image = sample['image']\n",
    "question = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "ground_truth = sample['answer']\n",
    "\n",
    "predicted = generate_answer(model, image, question, tokenizer, device)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground Truth: {ground_truth}\")\n",
    "print(f\"Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae810e41",
   "metadata": {},
   "source": [
    "## 10. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the model\n",
    "files.download(config.save_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
